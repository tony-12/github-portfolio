# -*- coding: utf-8 -*-
"""Transformers_pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1unvopIoYXMt-YeNLmhPm1-eZegOkQoRQ
"""

from transformers import pipeline
pp = pipeline('sentiment-analysis')

ppt = pipeline('text-classification')

gd = pipeline('zero-shot-classification')

pd = pipeline('question-answering')
st = pipeline('summarization')



pp('I hate each banana')

st('I hate eating banana everyday')

# TEXT GENERATION

generate = pipeline('text-generation', model='gpt2')
generate('I hate eating banana everyday', max_length=30, num_return_sequences=2)

generate('I love you more than words and wish you are here with me ', max_length=30, num_return_sequences=2)

# prompt: FILL THE MASK

from transformers import pipeline
pp = pipeline('sentiment-analysis')

ppt = pipeline('text-classification')

gd = pipeline('zero-shot-classification')

pd = pipeline('question-answering')
st = pipeline('summarization')
pp('I hate each banana')
st('I hate eating banana everyday')
# TEXT GENERATION
generate = pipeline('text-generation', model='gpt2')
generate('I hate eating banana everyday', max_length=30, num_return_sequences=2)
generate('I love you more than words and wish you are here with me ', max_length=30, num_return_sequences=2)

#FILL MASK
fill_mask = pipeline("fill-mask", model="bert-base-uncased")
fill_mask("Hello I'm a [MASK] model.")



# RESUME SCREENING WITH NLP MODEL

"""# New Section"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

data = pd.read_csv('/content/UpdatedResumeDataSet.csv')
data.head()

data.shape

# DATA EXPLORATION

data['Category'].value_counts()

plt.figure(figsize=(15,8))
sns.countplot(data['Category'],palette='tab10',order=data['Category'].value_counts().index)
plt.xticks(rotation=60)
plt.show()

# Checking the uniqueness of each variables
uniq = data['Category'].unique()
uniq

count = data['Category'].value_counts()
labels = data['Category'].unique()

plt.figure(figsize=(15,9))
plt.pie(count,labels=labels,autopct='%1.2f%%')#colors=plt.cm.coolwarm(np.linspace(0,1,3)))
plt.show()

#EXPLORING THE RESUME

data['Category'][0]

data['Resume'][0]

# DATA CLEANING (URL'S, HASHTAGS, MENTIONS,SPECIAL LETTERS, PUNCTUATIONS )

import re
import string

def clean_text(text):
    text = re.sub('http\S+\s*', ' ', text)  # remove URLs
    text = re.sub('RT|cc', ' ', text)  # remove RT and cc
    text = re.sub('#\S+', '', text)  # remove hashtags
    text = re.sub('@\S+', '', text)  # remove mentions
    text = re.sub('[%s]' % re.escape("""!"#$%&'()*+,-./:;<=>?@[\]^_`{|}~"""), '', text)  # remove punctuations
    text = re.sub('\s+', ' ', text)  # remove extra whitespace
    text = text.lower()  # convert text to lowercase
    text = text.strip()  # remove leading/trailing whitespace
    text = re.sub(r'\d+', '', text)  # remove digits
    text = text.translate(str.maketrans('', '', string.punctuation))  # remove punctuation
    text = re.sub(r'[^\x00-\x7F]+', '', text)  # remove non-ASCII characters
    text = re.sub(r'[^\w\s]', '', text)  # remove special characters
    text = re.sub(r'\s+', ' ', text)  # remove extra whitespace


    return text

data['Resume'] = data['Resume'].apply(lambda t: clean_text(t))

data['Resume'][0]

## OR CAN USED THIS CODE FOR THE SAME WORK

#data['Resume'] = data['Resume'].apply(clean_text)

data.head()

# CONVERTING WORDS INTO CATEGORICAL VALUES

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
data['Labels'] = le.fit_transform(data['Category'])

data.head()

data['Category'].unique()

data['Labels'].unique()

## VECTORIZATION(FEATURE EXTRACTION)

from sklearn.feature_extraction.text import TfidfVectorizer
tf = TfidfVectorizer(stop_words='english')
x = tf.fit_transform(data['Resume']) # Assiging the dataset into x and y
y = data['Labels']

x.shape,y.shape

## DATASET SPLITING

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=42)

x_test.shape,y_test.shape

x_train.shape,y_train.shape

## NOW I HAVE TO TRAIN THE MODEL AND PRINT OUT THE CLASSIFICATION REPORTS

from sklearn.metrics import classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.multiclass import OneVsRestClassifier
from sklearn.metrics import accuracy_score
lrm = OneVsRestClassifier(LogisticRegression())
lrm.fit(x_train,y_train)
y_pred = lrm.predict(x_test)
print(y_pred)
print(classification_report(y_test,y_pred))
print(accuracy_score(y_test,y_pred))
#lr = LogisticRegression()
#lr.fit(x_train,y_train)
#y_pred = lr.predict(x_test)
#print(y_pred)
#print(classification_report(y_test,y_pred))



# CREATING A PREDICTION SYSTEM FRO THE MODEL AND THE VECTORIZATION

ddd = """As a cyber-security analyst with over
 three years of experience, I specialize in threat analysis,
 risk mitigation, and network security, achieving an 85% reduction in cyber-attack attempts.
 I excel at identifying vulnerabilities, conducting thorough threat assessments,
 and implementing mitigation strategies, including advanced malware threat detection that
  reduced phishing incidents by 80% and uncovering over 100 security gaps through
  rigorous network audits. My technical expertise includes network system analysis,
  leveraging on machine learning models for static and dynamic predictions.
  Five years as an independent security researcher,
  I am skilled in penetration testing across diverse networks using Kali,
  Ubuntu, and Parrot OS. I have used advanced tools such as Nessus,
  SIEM and Vulnerability Scanners to mitigate attack vectors. My proficiency in data
  engineering and large-scale analysis in data science enables me to create
  detection and preventive models, supporting AI-driven innovations,
  Committed to excellence. I aim to contribute my expertise to safeguard and drive
  operational success
  SKILLS
  Cyber security, AWS Cloud Security, Burp, Coverity, SIEM,
  CISSP, Cryptography, Linux OS, Nessus, SET, Metasploit,
  Nmap, Penetration Tester, Splunk, Ethical Hacking,
  Risk Analysis, Firewall and IDS/IPS Configurations,
  Data Analytics, Data Exploration, Data Visualization,
  Machine learning and Artificial Intelligence, Neural Networks and
  Convolutional Neural, Numpy, Pandas, Matlibplot, PCA, SVM,
  Logistics regressions, Decision Tree, KNeighborsClassifier,
  Frontend and Backend Web Development, HTML, CSS, JavaScript,
  Power BI, Python, SQL,
  Tableau, DevOps."""

import pickle
pickle.dump(lrm,open('model.pkl','wb'))
pickle.dump(tf,open('transform.pkl','wb'))

#LOAD THE TRAINED MODEL
lrm = pickle.load(open('model.pkl','rb'))
tf = pickle.load(open('transform.pkl','rb'))

#CLEANED THE INPUT DATA
cn_d = clean_text(ddd)

#transform the cleaned data using the trained tfidfvectorizer
convert_feature = tf.transform([cn_d])

# PREDICTIONS USING THE LOADED MODEL
prediction = lrm.predict(convert_feature)[0]

#MAPPING THE CLABELS WITH CATEGORY
labels_names = {
    6: 'Data Science',
    12: 'HR',
    0: 'Advocate',
    1: 'Arts',
    2: 'Automation Testing',
    3: 'Blockchain',
    4: 'Business Analyst',
    7: 'Database',
    5: 'Civil Engineering',
    10: 'Electrical Engineering',
    11: 'Operations Manager',
    8: 'Health and fitness',
    9: 'Java Developer',
    13: 'PMO',
    14: 'Sales',
    15: 'Hadoop',
    16: 'Network Security Engineer',
    17: 'SAP Developer',
    18: 'Python Developer',
    19: 'SAP Products',
    20: 'Testing',
    24: 'Web Designing',
    21: 'DotNet Developer',
    22: 'SRE',
    23: 'DevOps'

    }
ct_name = labels_names.get(prediction)
print(ct_name,':',prediction)

!pip install streamlit -q

#!wget -q -O - ipv4.icanhazip.com

#! streamlit run ppee.py & npx localtunnel --port 8501





#DATA VISUALIZATION

"""IMPORT AND READ DATA

"""

#

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
plt.style.use('ggplot')
pd.set_option('display.max_columns',None)
pd.set_option('display.max_rows',None)

dt = pd.read_excel('/content/Fury_Friends data set_4376.xlsx')

dt.head()

"""UNDERSTAND THE IMPORTED DATA

data shape
dtypes
describe
"""

dt.shape

dt.head()

"""LISTING ALL THE COLUMNS IN THE DATASET"""

dt.columns

#Checking the data-types in the dataset
dt.dtypes

#Describe to list the mathematical expression of the dataset
dt.describe()

"""DATA PREPERATION

#Dropping irrelevant columns and rows
#Identifying duplicated columns
#Renaming Columns
#Feature Creation
"""



"""Removing inrelevant features"""

# Drops some of the columns that seems not fitting to the analysis
dt = dt[[#'Managers First Name',
    'Managers Surname', 'Area', 'Pet', 'Units Sld',
       'Revenue', 'Cost', 'Profit', 'Date']].copy()

"""Feature Creation"""

#dt['Date'] = pd.to_datetime(dt['Date']) this is to convert the date to datetime.
dt['Year'] = dt['Date'].dt.year
dt['Month'] = dt['Date'].dt.month
dt['Day'] = dt['Date'].dt.day
#dt['Week'] = dt['Date'].dt.week
dt['WeekDay'] = dt['Date'].dt.day_name()
dt['Net_Profit'] = dt['Revenue'].sum() - dt['Cost'].sum()
dt['Gross_Net_Profit'] = dt['Revenue'] - dt['Cost']



dt.dtypes

dt.head()

"""RENAME COLUMNS"""

#REMANE COLUMNS

dt.rename(columns={'Units Sld':'Units','Managers Surname':'Surname'},inplace=True)

dt.head()

"""FINDING MISSING VALUES AND HOW OFTEN THEY OCCUR IN THE DATASET"""

dt.isna().sum()



"""Again we have to check if any of our data values are duplicated

"""

dt.loc[dt.duplicated(subset=['Surname'])]

dt.loc[dt.duplicated().sum()]

dt.query('Surname =="Cocks"')

dt.duplicated().sum()

"""This code is to only detect rows in the datasets that are not duplicated"""



#WE ARE CHOOSING THE SELECTED SUBSET OF COLUMNS AS THE MAIN PLACE TO CHECK OUR DUPLICATES FROM
dt= dt.loc[~dt.duplicated(subset=['Surname','Area','Pet','Profit','Month'])] \
    .reset_index(drop=True).copy()

dt.isna().sum()



"""FIXING NULL VALUES FOR CATEGORICAL VALUES"""

from sklearn.impute import SimpleImputer
#imputer = SimpleImputer(strategy='mean')
for i in ['Units','Revenue','Cost','Profit','Gross_Net_Profit']:
  imputer = SimpleImputer(strategy='mean')
  dt[i] = imputer.fit_transform(dt[[i]])

# @title Default title text
dt.isna().sum()

# impute Graduated and Family_Size features with most_frequent values
from sklearn.impute import SimpleImputer
impute_mode = SimpleImputer(strategy = 'most_frequent')
impute_mode.fit(dt[['Surname','Pet']])

dt[['Surname','Pet']] = impute_mode.transform(dt[['Surname','Pet']])

dt.isna().sum()

dt.duplicated().sum()

"""UNDERSTANDING INTO THE DATASET BASED ON THE FEATURE CHARACTERISTICS

(Univariate Analysis)

. plotting feature distributions

. Histogram

. KDE

. Boxplot

"""

#Plotting the WeekDays in section to the dataset distribution
dt['WeekDay'].value_counts()\
.plot(kind='bar',color='red',title='Top WeekDays in the dataset for sales',figsize=(15,8))
plt.xticks(rotation=90)
plt.ylabel('Count')
plt.show()

plt.figure(figsize=(15,8))
sns.countplot(dt['Area'],palette='tab10',order=dt['Area'].value_counts().index)
plt.xticks(rotation=90)
plt.show()

plt.figure(figsize=(15,8))
sns.countplot(dt['Pet'],palette='tab10',order=dt['Pet'].value_counts().index)
plt.xticks()
plt.yticks()
plt.show()



count = dt['Area'].value_counts()
labels = dt['Area'].unique()

plt.figure(figsize=(15,9))
plt.pie(count,labels=labels,autopct='%1.2f%%')#colors=plt.cm.coolwarm(np.linspace(0,1,3)))
plt.show()

dt.groupby('Pet')['Gross_Net_Profit'].sum().sort_values(ascending=False)

# Sum Total for the Profit Made from respective Pet sold
#dt.groupby('Pet')['Gross_Net_Profit'].sum().sort_values(ascending=False)
plt.figure(figsize=(15,8))
sns.barplot(x=dt.groupby('Pet')['Gross_Net_Profit'].sum().sort_values(ascending=False).index,
            y=dt.groupby('Pet')['Gross_Net_Profit'].sum())

pie = dt.groupby('Area')['Gross_Net_Profit'].sum().sort_values(ascending=False)
plt.figure(figsize=(15,8))
plt.pie(pie,labels=pie.index,autopct='%1.2f%%')
plt.show()



dt.head()

dt['id_count'] = dt.groupby('Surname').cumcount() + 1
dt.head()

import plotly.express as px
import plotly.graph_objects as go
import plotly.figure_factory as ff
from plotly.subplots import make_subplots


def generate_rating_df(dt):
    rating_df = dt.groupby(['Area', 'Pet']).agg({'id_count': 'count'}).reset_index()
    rating_df = rating_df[rating_df['id_count'] != 0]
    rating_df.columns = ['Area', 'Pet', 'counts']
    rating_df = rating_df.sort_values('Pet')
    return rating_df
rating_df = generate_rating_df(dt)
fig = px.bar(rating_df, x='Area', y='counts', color='Pet',\
             barmode='group',text='counts')
fig.update_traces(textposition='auto',textfont_size=35)
fig.show()

## Count the total number of product in Category
def sales_count(axx):
    for i in axx.patches:
        wag = i.get_width()
        x = i.get_x() + i.get_width()
        y = i.get_y() + i.get_height()/2
        axx.annotate(round(wag,2),(x,y))

plt.figure(figsize=(15,5))
ax0 = sns.countplot(data = dt, y ='Area',hue='Area',
                  order =dt['Area'].value_counts().index)

sales_count(ax0)
plt.show()

dt.drop('Date',axis=1,inplace=True)
dt.info()

def profit_sub(af):
    for i in af.patches:
        height = i.get_width()
        x = i.get_x() + i.get_width()
        y = i.get_y() + i.get_height()/2
        af.annotate(round(height,2),(x,y))

plt.figure(figsize=(10,5))
ax1 = sns.countplot(data = dt, y ='Area',palette='Set2',
                  order =dt['Area'].value_counts().index)

profit_sub(ax1)
plt.show()

## Finding all Profit made from each Store Location
p_sub = dt.groupby('Area').sum().reset_index()
p_sub[['Area','Profit']].sort_values('Profit',ascending=False)
plt.figure(figsize=(10,5))
ax1 = sns.barplot(data = dt, x='Profit', y ='Area',hue='Area',estimator='sum',
                  order =dt.sort_values('Profit',ascending =False)['Area'], errorbar='None',
                  ci =None,palette='husl')
ax1.bar_label(ax1.containers[0],fontsize=10)
profit_sub(ax1)
plt.show()

## Finding all Profit made from each item in the Sub-Category
p_sub = dt.groupby('Area').sum().reset_index()
p_sub[['Area','Gross_Net_Profit']].sort_values('Gross_Net_Profit',ascending=False)
plt.figure(figsize=(10,5))
ax1 = sns.barplot(data = dt, x='Gross_Net_Profit', y ='Area',hue='Area',estimator='sum',
                  order =dt.sort_values('Gross_Net_Profit',ascending =False)['Area'], errorbar='None',
                  ci =None,palette='husl')
ax1.bar_label(ax1.containers[0],fontsize=10)
profit_sub(ax1)
plt.show()

##Checking the sum of each item Sales from Sub-Category
p_sub = dt.groupby('Area').sum().reset_index()
p_sub[['Area','Revenue']].sort_values('Revenue',ascending=False)
plt.figure(figsize=(10,5))
ax11 = sns.barplot(data = dt, x='Revenue', y ='Area',hue='Area',estimator='sum',
                  order =dt.sort_values('Revenue',ascending =False)['Area'], errorbar='None',
                  ci =None,palette='husl')

ax1.bar_label(ax11.containers[0],fontsize=10)
profit_sub(ax11)
plt.show()

##Checking the sum of revenue obtained from each Pet
p_sub = dt.groupby('Pet').sum().reset_index()
p_sub[['Pet','Revenue']].sort_values('Revenue',ascending=False)
plt.figure(figsize=(10,5))
ax11 = sns.barplot(data = dt, x='Revenue', y ='Pet',hue='Pet',estimator='sum',
                  order =dt.sort_values('Revenue',ascending =False)['Pet'], errorbar='None',
                  ci =None,palette='husl')

ax1.bar_label(ax11.containers[0],fontsize=10)
profit_sub(ax11)
plt.show()

#Checking the sum of Profit made from Pet
p_sub = dt.groupby('Pet').sum().reset_index()
p_sub[['Pet','Profit']].sort_values('Profit',ascending=False)
plt.figure(figsize=(10,5))
ax11 = sns.barplot(data = dt, x='Profit', y ='Pet',hue='Pet',estimator='sum',
                  order =dt.sort_values('Profit',ascending =False)['Pet'], errorbar='None',
                  ci =None,palette='husl')

ax1.bar_label(ax11.containers[0],fontsize=10)
profit_sub(ax11)
plt.show()

#Checking the sum of each item Profit from Pet
p_sub = dt.groupby('Pet').sum().reset_index()
p_sub[['Pet','Gross_Net_Profit']].sort_values('Gross_Net_Profit',ascending=False)
plt.figure(figsize=(10,5))
ax11 = sns.barplot(data = dt, x='Gross_Net_Profit', y ='Pet',hue='Pet',estimator='sum',
                  order =dt.sort_values('Gross_Net_Profit',ascending =False)['Pet'], errorbar='None',
                  ci =None,palette='husl')

ax1.bar_label(ax11.containers[0],fontsize=10)
profit_sub(ax11)
plt.show()

#checking monthly sales in respect to Cost
sales_by_month = dt.groupby('WeekDay')['Cost'].sum().reset_index()
fig = px.line(sales_by_month,
              x='WeekDay',
              y='Cost',
              title='Cost-Per-Week-Analysis')
fig.show()

# Create grouped bars

bar_width = 0.25
x = np.arange(len(dt['Pet']))


plt.figure(figsize=(10, 6))
plt.bar(x - bar_width, dt['Revenue'], width=bar_width, label='Revenue', color='skyblue')
plt.bar(x, dt['Cost'], width=bar_width, label='Cost', color='orange')
plt.bar(x + bar_width, dt['Profit'], width=bar_width, label='Profit', color='green')

# Customize chart
plt.xlabel('Area')
plt.ylabel('Amount (£)')
plt.title('Revenue, Cost, and Profit Comparison per Area')
plt.xticks(ticks=x, labels=dt['Pet'])
plt.legend()
plt.tight_layout()
plt.grid(axis='y', linestyle='--', alpha=0.7)

# Show chart
plt.show()

# Set up the plot
fig, axs = plt.subplots(1, 3, figsize=(15, 5))

# Pie chart for Revenue
axs[0].pie(dt['Revenue'], labels=dt['Pet'], autopct='%1.1f%%', startangle=140)
axs[0].set_title('Revenue Distribution')

# Pie chart for Cost
axs[1].pie(dt['Cost'], labels=dt['Pet'], autopct='%1.1f%%', startangle=140)
axs[1].set_title('Cost Distribution')

# Pie chart for Profit
axs[2].pie(dt['Profit'], labels=dt['Pet'], autopct='%1.1f%%', startangle=140)
axs[2].set_title('Profit Distribution')

plt.tight_layout()
plt.show()



#checking monthly sales in respect to Profit
sales_by_month = dt.groupby('WeekDay')['Profit'].sum().reset_index()
fig = px.line(sales_by_month,markers=True,
              x='WeekDay',
              y='Profit',
title='Profit-Per-Week-Analysis')
fig.show()

#checking the Gross_net_profit in respect to WeekDays
sales_by_month = dt.groupby('Gross_Net_Profit')['Month'].sum().reset_index()
fig = px.line(sales_by_month,
              x='Gross_Net_Profit',
              y='Month',
              title='Cost-Per-Week-Analysis')
fig.show()

#checking monthly sales
sales_by_month = dt.groupby('Area')['Profit'].sum().reset_index()
fig = px.line(sales_by_month,markers=True,
              x='Area',
              y='Profit',
title='Profit-Per-Week-Analysis')
fig.show()

#checking monthly sales
sales_by_month = dt.groupby('Pet')['Profit'].sum().reset_index()
fig = px.line(sales_by_month,markers=True,
              x='Pet',
              y='Profit',
title='Profit-Per-Week-Analysis')
fig.show()

#checking monthly sales
sales_by_month = dt.groupby('WeekDay')['Revenue'].sum().reset_index()
fig = px.line(sales_by_month, markers=True,line_dash_map={'Revenue':'dot'},color_discrete_sequence=['black'],
              x='WeekDay',
              y='Revenue',
              title='Revenue-Per-Week-Analysis')
fig.show()

def days_of_the_week(dt):
    rating_df = dt.groupby(['Pet', 'Month']).agg({'id_count': 'count'}).reset_index()
    rating_df = rating_df[rating_df['id_count'] != 0]
    rating_df.columns = ['Pet', 'Month', 'counts']
    rating_df = rating_df.sort_values('Pet')
    return rating_df
rating_df = days_of_the_week(dt)
fig = px.bar(rating_df, x='Month', y='counts', color='Pet', barmode='stack',text='counts',)
fig.update_traces(textposition='auto',textfont_size=35)
fig.show()

def day_of_the_week(dt):
    rating_df = dt.groupby(['Area','Profit']).agg({'id_count': 'count'}).reset_index()
    rating_df = rating_df[rating_df['id_count'] != 0]
    rating_df.columns = ['Area', 'counts','Profit']
    rating_df = rating_df.sort_values('Area')
    return rating_df
rating_df = day_of_the_week(dt)
fig = px.bar(rating_df, x='Area', y='counts', color='Area', barmode='stack',text='counts',)
fig.update_traces(textposition='auto',textfont_size=25)
fig.show()

# Melt the data for grouped bar chart
df = pd.melt(dt, id_vars=['Store', 'Pet'], value_vars=['Revenue', 'Cost', 'Profit'],
                    var_name='Metric', value_name='Amount')

# Combine Store and Pet_Type for x-axis grouping
df['Group'] = df['Store'] + ' - ' + df['Pet']

# Plot
plt.figure(figsize=(12, 6))
sns.barplot(data=df, x='Group', y='Amount', hue='Metric')

# Aesthetics
plt.title('Revenue, Cost, and Profit by Store and Pet Type')
plt.xlabel('Store and Pet Type')
plt.ylabel('Amount (€)')
plt.xticks(rotation=45)
plt.legend(title='Metric')

plt.tight_layout()
plt.show()

x = [1,2,4,5]
y = [[0.2,0.4,0.5,2.0],[0.9,0.4,0.1,1.0],[0.7,0.6,0.8,0.9],[1.3,0.9,1.7,0.7]]
years = [2018,2019,2020,2021]

brot = ['kofi','James', 'Peter','Cindy']
matt = [[1,6,8,3],[2,4,6,2],[4,6,7,4],[4,9,0,9]]


color = ['red','green','yellow','magenta']


dot = [2,4,6]

output = []
for i in brot:
  print(i)

for b, u in zip(brot,matt):
  print(b)
  print(u)

plt.figure(figsize=(10,6))

for a,(lang,rank) in enumerate(zip(brot,matt)):
  plt.plot(years,rank,label=lang,color=color[a],linewidth=2)

plt.gca().invert_yaxis()
plt.xticks(years,fontsize=10)
plt.yticks(range(1,5),fontsize=10)
plt.title("Programming language Trends",fontsize=14)
plt.xlabel("Years",fontsize=14)
plt.ylabel("Popularity",fontsize=14)
plt.legend(fontsize=14)
plt.show()

x = [1,2,4,5]
y = [[0.2,0.4,0.5,2.0],[0.9,0.4,0.1,1.0],[0.7,0.6,0.8,0.9],[1.3,0.9,1.7,0.7]]

bias = [1,0.4,3,9]

lay_output = []
for neuron_weights,neuron_bias in zip(y,bias):
  neuron_output = 0
for n_input,weight in zip(x,neuron_weights):
  neuron_output += n_input*weight
  neuron_output += neuron_bias
  lay_output.append(neuron_output)
print(lay_output)

poll = ['Peter','James','Aunty','Ruth','best']
count = {}
lop = ('we have the best game of all times.just imagine you having the best time and those time \
were the best of all.')
for x in lop.split():
  count[x] = count.get(x,0) + 1
print(count)

print(lop.count('best'))

import pandas as pd
import tabulate as t
for keys,values in count.items():
  #y = t.tabulate([[keys,values]],tablefmt='grid')
  #print(y)
  x = pd.DataFrame({'Words':keys,'Counts':values},index=[0])
  print(x)



import docx

#data = pd.read_csv('/content/ABSTRACT HYDRIOD.docx')
file = open('/content/ABSTRACT HYDRIOD.docx','r',encoding='Latin-1')
text = file.read()

print(text)





